my_identifier:
  # definition of the remote
  source:
    ssh:
      hostname: my.server.net

      # If left blank use username of current logged in user (SSH default behaviour)
      username: my_dump_user

      # If you login via password you must declare it here (prompt will break).
      # Consider using key authentication and leave the password blank.
      password: my_secret

      # Can be a string or an array of strings.
      # Can be absolute or relative to the _location of this file_ (or start with ~)
      # If left blank Net::SSH will attempt to use your ~/.ssh/id_rsa automatically.
      keyfile: ~/.ssh/id_rsa

      # The remote temp directory to place dumpfiles in. Must be writable by the
      # SSH user and should be exclusively used for this tool though cleanup only
      # removes .dbsc files. The directory must exist!
      tmp_location: /home/my_dump_user/db_sucker_tmp

    # Remote database settings. DON'T limit tables via arguments!
    adapter: mysql2 # only mysql is supported at the moment
    hostname: 127.0.0.1
    username: my_dump_user
    password: my_secret
    database: my_database
    args: --single-transaction # for innoDB
    client_binary: mysql # used to query remote server
    dump_binary: mysqldump # used to dump on remote server

  # define as many variations here as you want.
  variations:
    # define your local database settings, args will be passed to the `mysql` command for import.
    default:
      adapter: mysql2 # only mysql is supported at the moment
      database: tomatic
      hostname: localhost
      username: root
      password:
      args:
      client_binary: mysql # used to query/import locally

    # you can inherit all settings from another variation with the `base` setting.
    quick:
      base: default
      label: "This goes quick, I promise"
      only: [this_table, and_that_table]

    # you can use `only` or `except` to limit the tables you want to suck but not both at the same time.
    # there is also an option `ignore_always` for tables which you never want to pull (intended for base variation)
    # so that you don't need to repeat them in your except statements (which overwrites, no merge here)
    unheavy:
      base: default
      except: [orders, order_items, activities]
      ignore_always: [schema_migrations]

    # You can also copy the downloaded files to a separate directory (this is not a backup!)
    # - If database is also given perform both operations.
    # - If file suffix is ".gz" gzip file will be copied, otherwise we copy raw SQL file
    # - Path will be created in case it doesn't exist (mkdir -p).
    # - Path must be absolute (or start with ~) any may use the following placeholders:
    #    :date       2015-08-26
    #    :time       14-05-54
    #    :datetime   2015-08-26_14-05-54
    #    :table      my_table
    #    :id         981f8e2f278fa7f029399996b02e869ed8fd7709
    #    :combined   :datetime_-_:table
    with_copy:
      base: default
      file: ~/Desktop/:date_:id_:table.sql.gz
      gzip: true

    # only save files, don't import
    only_copy:
      base: with_copy
      database: false

    # You may use a different importer. Currently there are the following:
    #   binary     Use `dump_binary' executable
    #   void10     Sleep 10 seconds and do nothing (for testing/development)
    #
    # Mysql adapter supports the following flags for binary importer
    #   +dirty     Use `mysql' executable with dirty speedups (only deferred)
    dirty:
      base: default
      importer: binary
      importer_flags: +dirty

    # limit data by passing SQL constraints to mysqldump
    # WARNING:
    #   If a constraint is in effect...
    #     - mysqldump options get added `--compact --skip-extended-insert --no-create-info --complete-insert'
    #     - your local table will not get removed! Only data will get pulled.
    #     - you can have different columns as long as all remote columns exist locally
    #     - import will be performed by a ruby implementation (using Sequel)
    #     - import will ignore records that violate indices (or duplicate IDs)
    recent_orders:
      base: default
      only: [orders, order_items]
      constraints:
        last_week: &last_week "created_at > date_sub(now(), INTERVAL 7 DAY)"
        orders: *last_week
        order_items: *last_week
        # you can use the following to apply a query to all tables which get dumped:
        __default: *last_week

    # NOT YET IMPLEMENTED!
    # pseudo-incremental update function:
    # If you want to pull latest data you can either use constraints like above or use this dirty feature
    # to make it even more efficient. It will lookup the highest value of a given column per table and only
    # pulls data with a value greater than that. Typically that column is your ID column.
    #
    # WARNING:
    #   - you will get weird data constellations if you create some data locally and then apply a "delta"
    #   - records removed on the remote will still be there locally
    #   - this is just for the lazy and impatient of us!
    eeekkksss:
      base: default
      except: [table_without_id_column]
      incremental: true
      incremental_columns: # in case you want to override the `id' column per table (default is id)
        some_table: custom_id
        another_table: created_at # don't use dates, use unique IDs!
